{"pages":[{"title":"","text":"","link":"/about/index.html"}],"posts":[{"title":"Note-Recommender System","text":"Collaborative Filtering(協同過濾)approach使用&quot;wisdom of the crowd&quot;推薦物品 基本假設與想法 1.對於商品的ratings(implicitly or explicitly隱性或顯性) 2.客戶過去對某些商品有喜好，未來也會有相同的喜好 3.(網頁停留時間、點擊率) Input 使用者對商品的rating(很多使用者的ratings形成矩陣[user-item矩陣]) Output使用者喜歡或討厭那些商品 可預測每商品分數--推薦Top N個商品 以使用者為基礎的CF(User-based nearest-neighbor collaborative filtering)考慮: 1.相似度如何評估2.考慮多少鄰居 3.如何透過鄰居之分數來預測 目標使用者(active user)A: 透過相似的使用者(peers/nearest neighbor)來推薦與預測A喜歡的商品 用相似使用者之平均來預測是否A會喜歡商品 etc. Measuring user similarityPearson correlation(機率範圍介於-1到1) 與較相似的使用者權重可能會相當 Improve the metrics/prediction function 不一定每個neighbor的rate都為equally &quot;valuable&quot; 偏好不同導致 解決:變異度高的給權重高 case amplification 相似客戶與目標客戶相似度太高，將其權重放大 neighborhood selection 調整鄰居的數目 Content-based通常推薦text documents，如網頁、新聞需求:內容content、喜好preferences 目標:學習user之preferences，找到推薦用戶喜好&quot;相似&quot;的item Knowledge-basedConstraint-based約束(基於明確的推薦規則)找出需求後，看那些商品符合這些requirement 從這些條件找出能滿足最大條件的商品 Case-based(基於不同類型的相似性度量)如果與使用者需求不符 叫使用者改變需求 使用者自己去訂製(違背推薦系統) 使用者有特定需求，則推薦系統推薦有這種條件之物品 requirement都會有不同的權重(例如:買手機有些人一定要求相機要好) 解決:1.CSP (Constraint satisfaction Problem) 此問題可描述為(V, D, C) V: is a set of variables D: is a set of finite domains for these variables,and C: is a set of constraints that describes the combinations of values that variables can simultaneously take. 用crossover logic可解決 2.Conjunctive query Hybrid recommender systems共分別為下列三種 Parallel use of several systemsMonolithic exploiting different featuresPipelined invocation of different systems Feature combination strategy 可以用CF來推薦 但透過資料可以觀察到其他user items的偏好 再過濾出要推薦的商品 AI期刊：AAAI、IJCAI、AAMAS CVPR、ICCR、ECCR、ICML、ICLR、ACML、NIPS","link":"/2021/12/03/rs-note-01/"},{"title":"master-thises","text":"","link":"/2021/12/21/master-thises/"},{"title":"scenery-of-america","text":"Scenery of America-First WeekDay 1 - 2021.12.24這是我第一次來美國。上午11點帶著期待從桃園機場出發，到達舊金山時是24號上午6點，感覺上這一天有40個小時可以過，真的很神奇。聯合航空整體來說服務還算是挺好的，讓我印象比較深刻的是這次搭機只有50幾位乘客，我們每位乘客都可以有自己的一排，甚至在休息時可以直接在3至4個位置上躺下；還有他們的App算是讓我比較喜歡的，可以直接用他們的App來使用entertainment的所有項目。 Jake接到我們之後有去看金門大橋，回家我們後幫忙Patty用擀麵棍製作些手工藝，之後實在是撐不住就小睡了一覺。 起來後我們一起到Jake的外婆家慶祝聖誕節，大家人都非常nice，除了餅乾沙拉以外，我們吃了些像是玉米粥的東西還有咖哩飯，吃完飯後我們一起玩了些團康遊戲(Trivia, Hands Up)；外國的平安夜就跟台灣的農曆新年很像，家人們會聚在一起吃喝聊天，這次在美國過聖誕節氣氛很好，對我來說是個非常新鮮的體驗。 Day 2 - 2021.12.25第二天半夜四點多就醒來了，時差還沒有完全調整好。上午寫完我的Blog之後Jake找我一起去7-11買飲料，跟台灣挺不一樣的，有非常多的Monster Energy，Jake抓了幾瓶；我則選擇了一瓶Mountain Dew。回到家後Katie跟Clare已經在忙著做早餐，姊跟我說他們各個都是三明治專家；這三明治也真的超好吃，其內容物包含：酪梨、番茄、培根、蛋，我聽他們說這叫做American Scone。 中午時我們完成了昨天的手工藝品，Tom帶我去倉庫用圓鋸機把擀麵棍上方鉅出一條縫，好讓吸附磁鐵的鐵片可以嵌進去。Tom說它將會放在外婆家外當作裝飾品。 晚餐時間，大家今天聚在Tom家慶祝聖誕節，今天吃了沙拉、烤肉、燻火腿、馬鈴薯泥還有一道烤洋蔥豆子，我特別覺得豆子這道超級好吃，我吃完的時候直接痛哭流涕。 晚餐時間過後，大家在拍了大合照之後，Tom發給大家聖誕襪，裡面有美金以及口袋懷錶！ 在這之後我們進行了聖誕節的遊戲，我們將拆掉一個很大的保鮮膜球！這個保鮮膜球裡面捆了非常多的禮物，遊戲玩法是：大家會圍成一圈，保鮮膜球輪流傳給大家拆，骰子也是大家輪流骰，直到有人骰出Doubles才會將保鮮膜球傳給下一位，在還沒有人骰出Doubles的時候你都可以瘋狂的拆這顆禮物！昨天聽到Joe稱這顆球為Chaos，非常搞笑。我拿到了一個防水袋、一包電池、隨身充、魚餌、還有一根鹿角造型的鉛筆(整枝都是鉛筆)。 拆完禮物後，大家一起玩了Trivcia以及你畫我猜，直到累了回房間休息。第二天有比較身處於美國這件事情了，雖說跟大家對話還不是很流利，但是姊姊會幫助我，也在各種與大家互動的遊戲中得到了很多的練習。祝福各位聖誕快樂！ Day 3 - 2021.12.26今天上午跟Jake、Clare還有姐姐去吃了當地的早餐（Sunshine Cafe），我點了第一個在美國吃到的漢堡搭配番茄沙拉，整體來說是非常不錯的，黑咖啡也是可以一直續杯的。 吃完飯後跟Tom一家一起去逛了Barnes &amp; Noble還有REI，前者有點像是台灣的誠品，我在音樂區跟電腦科學區逛了很久，可惜沒有找到喜歡的書；後者像是體育用品店，賣了很多諸如登山、露營的用品與衣著，我在那邊買到一件不錯的背心以及一個軟包，很期待以後旅遊或是露營可以用到它們。 下午我們在家中與Jake的Cousin一起聚會，有非常多的小孩，大家都在聊天以及玩小孩玩狗，我則是坐在沙發上看美式足球，持續享受這這聖誕節的氣氛，大家也會到廚房完成自己想吃的三明治並且把他烤到你喜歡的程度，我覺得非常棒。 晚上時我們與Jake一家去玩了密室脫逃，是一個很棒的體驗，也是我第一次嘗試密室逃脫，超出我以前所想像的有趣。回到家後大家玩了drinking games，這遊戲稱之為baseball，的我們分成了兩組，在桌上兩隊都會有四個杯子排成一排，丟進這四個杯子的意義由近至遠就是一壘至全壘打；以及六個杯子分別代表兩隊的壘包。同時當你在壘包上、你可以選擇快速的喝一口酒，之後開始跟你隔壁隊壘包的防守員PK把杯子從倒立變成正立，功方獲勝代表盜壘成功，反之則出局。 第三天在這邊慢慢觀察到些美國生活的特別之處，好比加州是可以紅燈右轉的除非有告示牌制止；在自家門前傳接美式足球是一件很正常的事情；每個人在這邊都很進入狀況，都很認真的在生活。 Day 4 - 2021.12.27上午從一個平凡的美式早餐開始，大家吃著cereal搭配咖啡還有柳橙。今天上午的行程Tom帶著大家前往Berkeley，我們首先來到UC Berkeley，天氣不是很好所以並沒有拍了很多照片，比較可惜的是沒有拍到Sather Tower的照片。這是我第一次參觀美國的學校，覺得環境還是挺不錯的，有點像是高配版的台灣大學學校。 繞完學校後我們到附近的Berkeley Rose Garden轉轉，是個風景很美的地方，從遠處也可以看見SFO。大家在一旁的滑梯玩了一陣子，非常開心，而當然剛剛說天氣不好地板非常濕，所以玩完滑梯的同時我們的褲子也都濕掉了。 離開之後我們到Berkeley Waterfront走了一圈，這時天氣已經放晴，也在此停留一陣子並且拍了許多照片，隔著SFO Bay看過去的景色還算是非常令人驚豔的。 中午的時候我們在Avalon Public Market吃中餐，大致上像是一個高級的美食街？裡面有賣許多料理，包含拉麵、韓式烤肉、咖哩等，我則是點了Tacos。吃飽回程途中我們有經過Pixar，可惜的是只能從外面瞥見裡面的Pixar球跟檯燈，並沒有開放參觀。 Day 5 - 2021.12.28今天上午從Starbucks開始，展開奇妙的一天。上午Tom帶上我跟Jake還有Joe一起去採買磚塊，為的是要去幫外婆家新蓋的紡織屋鋪磚塊，我們在Lowe’s(美國的五金行)買到磚塊，這磚塊大概是30幾磅，所以今天我有很足夠的運動量。 外婆很開心我們能來幫忙，她笑得非常開心，我也很期待這項任務完成的那天！值得一提的是，外婆家的院子充滿狗屎，今天是我的Stan Smith最骯髒的一天。外婆家在Pittsburgh，那邊是個好地方，有許多小山丘，在那邊生活的人看起來都很Chill。 中午我跟Jack一起到Southern Comfort買午餐，我們吃了些漢堡、穀物等，其實這幾天吃下來我覺得美國的食物還算是蠻重口味的而且偏乾，總是會想在吃飯時多喝點水。 晚上的時候，我跟Jake幫忙姊搬家，到她即將入住的地方(Sunnyvale)，附近看起來挺方便的，姊的室友Julie人也非常nice，祝福之後她在那邊的生活與工作能一切順利。 在那之後我們也到姊搬家期間Waymo提供給她的住處看了一下，我的感想是，選對公司很重要，並且這種福利跟台灣公司真的沒辦法做比較。來到美國已經第五天，從一些細節上，讓我覺得在這生活是一件非常棒的事情。 Day 6 - 2021.12.29還記得Tom家前的枯樹嗎？今天上午，Tom跟我還有Jake一起把他給砍了！我們大概花了三四個多小時，反覆的把土挖出來並且鉅掉根。最後是用車子後方與樹的頂端綁上繩子把樹拉倒的，這一切都太瘋狂了，我從來沒做過種種事情。 我們在復原完畢後鋪上了兩大袋的草木灰，除了我的外套跟牛仔褲髒透了以外，一切就像甚麼都沒發生一樣。 在那之後的午餐我非常餓，我把一個很巨大的漢堡給吃掉了。 晚上的時候我跟姊還有Jake一起去Scott家玩PathFinders。其實我在來美國之前很少嘗試著大家一起玩桌遊，一部份的原因是因為那些看似非常複雜的設定，另外我也非常懶得去了解它，但我能說它真的很好玩，而且Scott是個很有趣的人，它把遊戲形容得繪聲繪影，就像出門前我問姊我們是要去玩甚麼遊戲時她回答的一樣：想像遊戲。當然我聽到姊跟我這麼說的時候我的表情長這樣？_？ Day 7 - 2021.12.30San Francisco.過了Oakland Bay Brudge我們首先到了Coit Tower從高處享受著被SFO吞噬的感覺，通往Coit Tower頂端我們需要搭乘一個很復古的電梯，相當有特色。 在那之後我們來到Pier 39，是漁人碼頭這邊的一個旅遊中心、購物景點，除了有很多小店以外，很有特色的是那邊有以個海洋哺乳動物中心，旁邊的平台上全是海獅。中午時我們也在那邊的Bistro用餐，我點了很有特色的Boudin Sourdough Breadbowl，這個份量真的是大的可怕，吃完後有後悔沒有點Petite。 在碼頭逛完之後我們去看了Golden Gate Bridge，這景象真的非常壯觀，這可能就是來到舊金山的感受。 最後我們來到唐人街，裡面夾雜著許多日本、韓國、中國的店，挺新鮮的，雖說讓邊的交通讓我感受到美國的大都市的擁擠以及緊湊的氣氛，我並沒有很喜歡。舊金山還有一大特色是，街道真的非常的長直，並且是在大斜波上面，從車上或是街道上，你都可以看到遠處的街景。 晚上姊幫大家點了BBQ一起吃，這週真的非常感謝Tom一家人可以這麼的照顧我還有姊，雖然我總是很害羞，但他們都非常有耐心的與我溝通，在未來我也非常歡迎你們來台灣玩。","link":"/2021/12/25/scenery-of-america/"},{"title":"tcse-2020","text":"TCSE-2020 Learning to Rank for Collaborative Filtering前言這是一篇關於我在Mars Lab時發表於TCSE-2020的論文，首先要感謝邱志義教授以及學長Benny與我的partner Jim，總是在研究上給我許多的幫助以及啟發。這是一篇關於推薦系統的論文，有興趣的夥伴歡迎交流指教。 Motivation推薦系統現在廣泛應用於各個領域甚至是我們生活之中，其中有兩大要素啟發了我們這次的實驗。第一： 為了實現推薦系統，我們可以很直觀地認為系統應推薦符合用戶期望的物品。然而，給定一組物品，過去的研究時常專注於“相關”或“不相關”的某些物品，而不是專注於物品對之間的偏好程度。這可能違反推薦系統的主要目的。第二： leave-one-out evaluation protocol被廣泛應用如下： 對於每個用戶，他們保留他/她最近的交互作為測試集，並利用剩餘的數據進行訓練。由於在評估過程中為每個用戶對所有項目進行排序太耗時，他們遵循通用策略，隨機抽取 100 個用戶未交互的項目，在 100 個項目中對測試項目進行排名。但是，每個用戶最近的交互項可能是也可能不是他們喜歡的，有可能用戶對最新項的真實評分很低，不應該被系統推薦；它會與評估設置的設計相衝突。 I. Introduction&emsp;&emsp;The recommendation system is used to predict the user’s “rating” or “preference” of items and can help people cope with complex preferences prediction problems. Collaborative filtering approaches build a model from a user’s past behavior (items previously purchased or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict or rate items that the user may have an interest in. Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties. &emsp;&emsp;In 2017, Hseih et al. [1] proposed the collaborative metric learning (CML) which revealed the potential of inner product to model user-item relationships. They argued that the inner product violates the triangle inequality which is essential to model the fine-grained preference of users. Thus, the authors come up with a metric-based learning scheme that minimizes the distance between user and item vectors of positive interactions. Simultaneously, this scheme also learns user-user similarity and item-item similarity in the latent space. Their work demonstrates a highly competitive performance on many benchmark datasets. &emsp;&emsp;However, in spite of the success of these works, they consist some weaknesses. Firstly, for implementing a recommender system, it is intuitively to consider that the system recommends the item that fits the expectation of the user. However, given a set of objects, they focus on the certain pairs of objects that are “relevant” or “irrelevant” instead of knowing the preference degree between pairs of objects. It may violate the main purpose of the recommender system. Secondly, the leave-one-out evaluation protocol is widely adopted as follows: For each user, they held-out his/her latest interaction as the test set and utilized the remaining data for training. Since it is too time-consuming to rank all items for every user during evaluation, they follow the common strategy that randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. However, the latest interaction item of each user may or may not be the one they prefer, it is possible that the true rating of the latest item rated by the user is low, and should not be recommended by the system; it will conflict the design of the evaluation setup. &emsp;&emsp;To solve the above problem, we propose a novel collaborative metric learning method with respect to the traditional collaborative filtering, which learns a metric space to represent not only the user’s preferences but also their similarities. We employ the triplet loss function in neural networks to train users’ preference ranking model. We care more about user preferences than whether users interact with the item. Moreover, we come up with a new leave-one-out evaluation methodology. Rather than using his/her latest interaction as the test set, we hold-out one highest rated item of each user according to the true ratings then randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. This new methodology can retrieve the relevant item for the user and eliminate the conflict in the original evaluation. Experiment results show a great potential of our model compared to some state-of-the-art algorithms. &emsp;&emsp;The remainder of this paper is organized as follows. In Section II, we introduced the background used in this paper. In Section III, we introduce our novel collaborative matric learning model. In Section IV, we compare our method to some state-of-the-art algorithms on MovieLens dataset. Finally, we conclude this paper in Section V. II. BackgroundA. Feedbacks&emsp;&emsp;In order to implement a recommender system, it requires substantial data provided by the user purposely or unpurposely, also known as feedbacks. Feedbacks can be divided into the explicit feedback and implicit feedback by the differences of its essence. &emsp;&emsp;The explicit feedback directly shows the preferences of user, according to the opinion or score given to the item, usually formed as measurable indicators. Comparing to the explicit feedback, the implicit feedback is relatively blurry, it could be any slight action that user makes (e.g., clicks, browse, bookmarks). The definition of the implicit feedback can be described as follows: $$ y_{ui} =\\left\\{ \\begin{aligned} &1,if \\quad interaction < user_u, item_i > exist\\\\ &0, otherwise \\end{aligned} \\right. $$ where Y is the user-item interaction matrix. &emsp;&emsp;Note that for the implicit feedback, a value of 0 does not necessarily imply negative feedback. In most cases, the user is unaware of the existence of the item and this forms the keystone of the recommendation problem, i.e., if the user did not rate an item or have no interaction with an item, we can’t be sure the relation between them. B. Triplet Loss&emsp;&emsp;Triplet loss [2] is a widely used loss function in solving the optimization problem, it is even used to accomplish information retrieval tasks within different modalities in recent studies. Over all, triplet loss is a fine mapping technique and a good way to compute loss in the field of pattern recognition and cluster analysis. Figure 1 shows how triplet works. &emsp;&emsp;Triplet includes an anchor point, a positive point and a negative point. Through an embedding architecture, inputs are embedded onto the new vector space, where the embedded result follows the triplet loss concept. During the learning phase, the positive point will be pulled toward the anchor point while the negative point will be push away from the anchor point. Note that in our work, the anchor point indicates the user, positive point indicates the item that user prefers more and the negative point indicates the item that user prefers less. Figure 2 and Figure 3 show the embedded status before and after the learning. C. Collaborative Filtering (CF)&emsp;&emsp;Traditional CF algorithms [3] are based on the user similarity computed by heuristic, such as cosine similarity. Recommendations are made by aggregating the ratings of k-nearest users. Matrix factorization (MF) [4] has been the most popular CF approach for a decade due to its remarkable performance. The original MF models are designed to model users’ feedback by mapping users and items to a latent space, such that user-item relationships can be inferred through their dot product in this latent space. To be more precise, let \\(r_{jk}\\) denote user j‘s rating to item k, we learn user vector \\(u_j\\) and item vector \\(i_k\\) such that their dot product \\(u^T_j i_k\\) approximate \\(r_{jk}\\) through the following formula: $$ min_{u_* i_*} \\sum_{r_{jk} \\in K} (r_{jk}-u^T_j i_k)^2+ \\theta_1 \\Vert u_j \\Vert + \\theta_2 \\Vert i_k \\Vert $$ where K is the set of known ratings; \\(\\theta_1\\) and \\(\\theta_2\\) are hyperparameters that regularize the \\(L^2\\)-norm of \\(u_*\\) and \\(i_*\\). D. Leave-one-out&emsp;&emsp;Leave-one-out is a common evaluation protocol in recent recommender system papers [5] [6] [7] [8]. It holds-out one data from the original dataset for testing and the rest remains for training. Specifically, for recommender systems, according to the rated items’ timestamps of each user, the most recent rated item is held-out as the test set and the remaining items for training. Since it is too time-consuming to rank all items for every user during evaluation, most studies follow the common strategy that randomly samples 100 items that are not interacted by the user and rank the test item among the 100 items. A sketch map of the leave-one-out evaluation protocol is shown in Figure 4. III. The Proposed Medel&emsp;&emsp;In this section, we introduce our novel collaborative matric learning model. The overall architecture is shown in Figure 5. Our model aims to map the user and item onto a latent space in which not only represents the user-user and item-item relationship but also the users’ preferences, so that the recommendation fits the users’ expectation. Let us begin with a simple high-level overview of our model: Before the data enters the embedding layer as input, a triplet \\(\\lt user , item^+ , item^- \\gt\\) has to be chosen from the user-item interaction matrix. We added a threshold t to the interaction matrix according to the true ratings. After a user is chosen, the corresponding \\(item^+\\) and \\(item^-\\) are then choosed under the condition that \\(item^+\\) has a true rating greater than t, in the meantime \\(item^-\\) either has a true rating lower than t or is a 0 in the interaction matrix. \\(\\lt user , item^+ , item^- \\gt\\) is then converted to dense vector representations using an Embedding Layer. Output \\(\\lt u , i^+ , i^- \\gt\\) represents the user vector, positive item vector and negative item vector respectively. Given \\(\\lt u , i^+ , i^- \\gt\\), our model maps users and items onto a latent space in which similar users are close to each other and so do similar items. Meanwhile, for each user, the items he/she prefers more are relatively closer than those he/she prefers less. Our model optimizes for the triplet loss \\( max ( 0 , (u-i^+)^2 - (u-i^-)^2 + \\alpha ) \\approx 0 \\) using an optimizer with Tensorflow backend where \\( \\alpha \\) is a margin we defined. A. Preprocessing&emsp;&emsp;In order to build our user-item interaction matrix, it is necessary that we parse the data from the dataset beforehand. Take the MovieLens dataset we employed for example, it provides the user contents, movie contents and the ratings. User contents and movie contents are for content-base [9] methods, for collaborative filtering [3], we concern the ratings instead. The ratings file contains the history ratings of movies that users rated through time. In this file, users and their corresponding movies are represented as IDs, ratings are represented as a number in the range of [10] [5] followed by the timestamps that ratings were given. By parsing the ratings file to the type we needed, we then build our user-item interaction for model input using the equation 3. An example of the ratings file is shown in Figure 6. B. Interaction Matrix&emsp;&emsp;Let Y be the user-item interaction matrix, t be the threshold for the true ratings. We build our user-item interaction matrix follows: $$ y_{ui} =\\left\\{ \\begin{aligned} &1,if \\quad interaction < user_u, item_i > exist \\quad and \\quad r>t\\\\ &0, otherwise \\end{aligned} \\right. $$ where r denotes the true rating for item i rated by user u. An example of the user-item interaction matrix that built from Figure 6 is shown in Figure 7. C. Embedding Layer&emsp;&emsp;In this section we bring up how and why we do the embedding. For starters, there are two main reasons we choose to add in the embedding layer. First, due to the high dimensionality and sparsity of our data, in order to save resources and time, we adopt embedding instead of one-hot encoding. Second, every embedded vector within a neural network will be updated during the training phase. This leads us to reveal the similarities between embedded data in multidimensional space. The structure of the embedding lookup table is shown in Figure 8.&emsp;&emsp;The embedding layer in our model accepts a triplet \\(\\lt user , item^+ , item^- \\gt\\) as an input which is chosen from Y. Inputs of users and items are represented as their ID. Through the embedding layer, their ID is converted into a d dimensional real-valued dense vector. The output of this layer is represented as \\(\\lt u , i^+ , i^- \\gt\\), which are the embedded vector of the user, positive item and negative item respectively. D. Optimization and Learning&emsp;&emsp;In this section we introduce how the weight matric in MLP being updated, the objective function in our training scheme. We adopt the triplet loss for optimization and the loss is defined as follow: $$ L= \\max ( 0 , (\\hat{u}-\\hat{i^+})^2 - (\\hat{u}-\\hat{i^-})^2 + \\alpha ) $$ where \\( \\alpha \\) is the margin and the weight matrix in MLP is updated through iterations by minimizing L. An illustration of how the optimization and learning update the weight of each layer in our model is shown in Figure 9. E. New Leave-one-out&emsp;&emsp;In order to evaluate our model, we come up with a new leave-one-out methodology. We hold-out one highest rated item of each user according to the true ratings, then randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. Figure 10shows an illustration of the new leave-one-out methodology proposed. With the new leave-one-out methodology, our model will be learned to rank high rated items for each user to top-k among other items and the recommendation is made according to the ranking results. IV. Performance Evaluation&emsp;&emsp;In this section, we evaluate our proposed model against other state-of-the-art algorithms and show the implement details along with the experimental result at the end of this section. A. Datasets and Environment&emsp;&emsp;MovieLens [11] is a widely adopted benchmark dataset for collaborative filtering in the application domain of recommending movies to users. We use three configurations of this benchmark dataset including MovieLens100K, MovieLens1M and MovieLens20M. The statistics of MovieLens datasets are reported in Table 1.&emsp;&emsp;Totally we evaluate our proposed method on three datasets with diverse size and interaction densities. Experiments are implemented under Windows 10 with an Intel(R) Core(TM) i7-6700 CPU, up to 64GB RAM and a NVIDIA GeForce GTX 1080Ti GPU. For compile environment we use Keras package with Tensorflow backend under a 3.7 version of Python. TABLE I. The statistics of MovieLens Datasets Dataset Interactions #Users #Items %Density ML100K 100K 1K 1.7K 6.3 ML1M 1M 6K 4K 4.2 ML20M 20M 13K 27K 0.5 B. Baselines and Evaluation Protocol Multi-layered Perceptron (MLP) is the baseline using neural architecture in [5], which the authors proposed to model the relationships between users and items. Bayesian Personalized Ranking (BPR) [6] is one of the strong CF baseline in early years that minimizes \\( \\sum i \\sum j , k - \\log \\sigma ( p_i^Tq_j - p_i^Tq_k ) + \\lambda_1 \\Vert p_i \\Vert^2 + \\lambda_2 \\Vert q_j \\Vert^2 \\), where \\( (p_i,q_j) \\) is a positive interaction and \\( (p_i,q_k) \\) is a negative sample. Neural Matrix Factorization (NeuMF) is a framework that combine MF with MLP to predict the user item rating [5]. &emsp;&emsp;For evaluation, we adopt our new leave-one-out evaluation methodology, i.e., the testing set comprises the highest rated item of all users. If there are same rating items, we randomly sample one within those items. Since it is too time consuming to rank all items for every user, we randomly sampled 100 items that have no interactions with the target user as [5][6] [7] [8] and rank the test item among these 100 items. Since our work is formulated as learning to rank, we determine the performance of our model based on the well-known normalized discounted cumulative gain (nDCG@10) [12] and Hit Ratio (H@10), where nDCG@10 represents the precision while H@10 represents the accuracy. C. Implement Details&emsp;&emsp;By tuning the hyperparameters, we select the model which has the best performance through epochs. We obtain the result that model on the test set when an epoch ends and the model is saved when the current test result is better than previous, so the best model can be easily acquired when the training is finished. Models are trained for a maximum of 200 epochs with early stopping strategy [13]. The dimensionality of user and item embedding d is tuned amongst {10, 50, 100, 200}. Batch size during the training phase is tuned amongst {16, 64, 256, 1024} according to the number of trainable parameters. The learning rate is tuned amongst {0.17, 0.017, 0.0017, 0.00017}. Models are optimized using the Adam optimizer [14]. For the network architecture of the baselines, we follow the original setup in their studies respectively. The margin \\( \\alpha \\) in our triplet loss function is tuned amongst {0.1, 0.2, 0.3, 0.4, 0.5} depending on the output distribution. Due to the output of our model is in certain dimensionality with normalized vector value, the value of \\( \\alpha \\) doesn’t affect the result much, we simply choose \\( \\alpha \\) that gives the best performance.&emsp;&emsp;For each user, there are totally 100 items to rank during the testing phase including the ground-truth item. The test data is input to our model in a form of triplet as our model requires, after the testing triplet \\(\\lt test _userID , test _itemID , test _itemID \\gt\\) was input, the model outputs the latent vector representations of users and items. Since the position of negative item in the triplet means nothing during the testing phase, we simply replace it by repeating \\(\\lt test _itemID \\gt\\) in order to maintain the shape of our input. When the performance computation for each user ended, all H@10 and nDCG@10 will be added together and divide by the total user count to get the final result. D. Experimental Results&emsp;&emsp;The experiment results of our proposed model and baselines on three of the MovieLens [11] datasets are shown in Table II to Table VII. From the tables, we infer that some of the competitor baselines fail to obtain stable ranking results due to the variety of data characteristic in different datasets while our model maintains a stabler performance.&emsp;&emsp;Table II and Table III show the evaluation result using the new leave-one-out methodology we proposed. Under the goal of finding potential high rated item for users, our model performs competitively on all datasets and obtains an outstanding performance on both H@10 and nDCG@10 among the baselines.&emsp;&emsp;Table IV and Table V show the evaluation result using regular leave-one-out, in which the last timestamp item of each user is held-out for testing. Under this evaluation protocol, the best recommendation may not be made for each user. In order to make a contradiction, we also come up with an additional evaluation result by holding-out the lowest rated item for each user to test, the result is shown in Table IV and Table V.&emsp;&emsp;To sum up, from Table II and Table VII, we can infer that our proposed model performs well when the high rated item is held-out for testing. It also indicates that we successfully rank the high rated item to top-k among other items. On the contrary, when the lowest rated item is held-out for testing, our proposed model performs badly due to making bad recommendations violates the purpose of our model. TABLE II. H@10 Results on the Movielens Datasets. (High Rated) H@10 #MLP #BPR #NeuMF #Ours ML100K 65.12 77.09 82.82 83.67 ML1M 59.64 74.52 86.18 88.86 ML20M 78.90 89.09 96.09 97.62 TABLE III. nDCG@10 Results on the Movielens Datasets. (High Rated) nDCG@10 #MLP #BPR #NeuMF #Ours ML100K 38.71 50.57 56.23 60.91 ML1M 32.72 49.46 61.60 66.03 ML20M 54.52 52.37 79.61 82.39 TABLE IV. H@10 Results on the Movielens Datasets. (Timestamp) H@10 #MLP #BPR #NeuMF #Ours ML100K 51.97 64.26 83.40 81.44 ML1M 46.58 62.17 64.27 63.44 ML20M 72.36 95.22 98.19 96.64 TABLE V. nDCG@10 Results on the Movielens Datasets. (Timestamp) nDCG@10 #MLP #BPR #NeuMF #Ours ML100K 32.85 39.57 59.30 53.35 ML1M 30.16 40.55 45.18 42.96 ML20M 53.20 58.75 77.10 66.49 TABLE VI. H@10 Results on the Movielens Datasets. (Low Rated) H@10 #MLP #BPR #NeuMF #Ours ML100K 46.82 58.96 66.70 45.17 ML1M 40.30 43.21 60.03 34.09 ML20M 65.45 84.31 93.65 78.44 TABLE VII. nDCG@10 Results on the Movielens Datasets. (Low Rated) nDCG@10 #MLP #BPR #NeuMF #Ours ML100K 27.01 34.58 40.40 25.09 ML1M 22.17 23.35 35.49 17.90 ML20M 47.51 55.23 68.34 45.05 V. Conclusion&emsp;&emsp;In this work, we employ the triplet loss in the neural network to optimize collaborative filtering and learn a latent space to represent the user-user similarity, item-item similarity and the user preference. We also come up with a new leave-one-out methodology to train and test our model to fit our hypothesis. Furthermore, our framework treats the user-item interaction in a different way, which is simple and reasonable. Experiment results show a great potential of our model compared to some state-of-the-art algorithms. V. Acknowledgement&emsp;&emsp;This research was supported by the Research Support Scheme of the Ministry of Science and Technology, grant no. MOST 106-2221-E-415-019-MY3. References [1] C.-K. Hsieh, L. Yang, Y. Cui, T.-Y. Lin, S. Belongie, and D. Estrin, “Collaborative metric learning,” in Proceedings of the 26th International Conference on World Wide Web, 2017. [2] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: a unified embedding for face recognition and clustering,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. [3] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-based collaborative filtering recommendation algorithms,” in Proceedings of the 10th International Conference on World Wide Web, 2001. [4] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques for recommender systems,” Computer, vol. 42, no. 8, p. 30–37, 2009. [5] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua, “Neural collaborative filtering,” in Proceedings of the 26th International Conference on World Wide Web, 2017. [6] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, “BPR: bayesian personalized ranking from implicit feedback,” arXiv preprint arXiv:1205.2618, 2012. [7] I. Bayer, X. He, B. Kanagal, and S. Rendle, “A generic coordinate descent framework for learning from implicit feedback,” in Proceedings of the 26th International Conference on World Wide Web, 2017. [8] X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua, “Fast matrix factorization for online recommendation with implicit feedback,” in Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2016. [9] M. J. Pazzani and D. Billsus, “Content-based recommendation systems,” Springer, 2007, p. 325–341. [10] P. Resnick and H. R. Varian, “Recommender systems,” Communications of the ACM, vol. 40, no. 3, p. 56–58, 1997. [11] F. M. Harper and J. A. Konstan, “The movielens datasets: history and context,” ACM Transactions on Interactive Intelligent systems (tiis), vol. 5, no. 4, p. 1–19, 2015. [12] K. Järvelin and J. Kekäläinen, “Cumulated gain-based evaluation of IR techniques,” ACM Transactions on Information Systems (TOIS), vol. 20, no. 4, p. 422–446, 2002. [13] R. Caruana, S. Lawrence, and C. L. Giles, “Overfitting in neural nets: backpropagation, conjugate gradient, and early stopping,” in Advances in Neural Information Processing Systems, 2001. [14] D. P. Kingma and J. Ba, “Adam: a method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014. [15] M. Carvalho, R. Cadène, D. Picard, L. Soulier, N. Thome, and M. Cord, “Cross-modal retrieval in the cooking context: learning semantic text-image embeddings,” in The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, 2018. [16] S. Ding, L. Lin, G. Wang, and H. Chao, “Deep feature learning with relative distance comparison for person re-identification,” Pattern Recognition, vol. 48, no. 10, p. 2993–3003, 2015.","link":"/2021/12/26/tcse-2020/"},{"title":"scenery-fo-america-week2","text":"Scenery of America-Second WeekDay 8 - 2021.12.31上午，Jake帶著我還有姊來去逛傢俱，為姊未來的住處提前作準備，首先我們看了一些擺設於房間、餐廳的傢俱，姊只是先把規格拍下，或許等到當地或是完成樣式比價等再做決定，之後我們去買了Mattress，姊挑了一組含兩個枕頭的床墊，我覺得這花得很值得，因為我也算是個很注重生活品質的人，很開心她選到了一個很不錯的床墊。 下午我們待在外婆家，一起玩桌遊還有吃外婆煮的濃湯，濃湯真的超好喝，蛤蜊濃湯與生蠔濃湯兩種我都嘗試了，雖說還是覺得生蠔的味道很腥，但是他們都超棒的，過了一個悠閒的玩狗玩桌遊吃吃餅乾點心的下午。 從外婆家回Tom家的途中，我們買了些啤酒，準備吃晚餐，我餐我們叫了些台灣菜來吃，我能說這真的是台灣人開的餐廳，因為不是台灣人不可能弄出這麼好吃的炒飯，大家在歡樂的氣氛當中一起遊玩，等待新年的到來。 新的一年對自己也有許多的期許，希望自己能變得更有自信、希望自己可以迎接每項挑戰。期許家人能夠一切平安，事事順心。或許2021稱不上是一個很棒的一年，這年當中，我完成了我的碩士學位、服完兵役，結束了一段四年半的感情，但同時也新交了許多志同道合的好友，從他們身上學習到了許多新事物。最後，期許新的一年能過上自己所嚮往的生活，感謝身邊的一切人、事、物。新年快樂！ Day 9 - 2021.01.01今天下午我們前往外婆家，大家一起前往Contra Loma Regional Park一起走走，我們大概走了五公里，邊走邊聊天、看看風景，沿途也看到許多來散步釣魚遛狗的人們，大家都很友善，見面會互相問”Happy new year!”。 走完之後回到外婆家，外婆拆了我們之前做給她的禮物，她笑得非常開心。在那之後我們也一起吃了晚餐。 Day 10 - 2021.01.02今天是忙著幫姊搬家的一天，上午我們就去採買了許多諸如浴廁廚房房間等用品，上午忙完之後，我跟姊開車去買了IN-N-OUT，結論是：他是個貨真價實的薯條店呢；吃第一口漢堡的時其實覺得跟麥當勞的大麥克差不了多少，但是後面咬到洋蔥跟其他配料的時候，就覺得這漢堡有點東西，不愧是台灣朋友們推薦我一定要來試試看的漢堡店；還有，我應該不會再點奶昔了，我覺得冰淇淋就是冰淇淋，不要假裝成奶昔，這樣我會很困擾。 吃完午飯後Tom送給了我上禮拜砍樹時，那刻樹的木材製作成的一個杯墊，我覺得非常感動，上面刻有我的名字與金門大橋的圖樣；另一面刻有Aldens’s family大家的名字。真的非常感謝Alden’s family的大家可以這麼的照顧我與姊，也很高興可以跟他們一起慶祝聖誕節以及新年。 下午我們在姊的床墊到達時間的同時，開車前來姊的新住處放東西，我們安裝好了床與電扇與燈。之後我們來到韓式料理店吃晚餐，發現吃亞洲菜還是挺開心的，可能是因為比較少吃到米飯？很大碗的石鍋拌飯幾乎被我吃光了。 Day 11 - 2021.01.03今天是我跟姊來到Mountain View的第一天，上午我們來到Trader Joe’s採買生活必需品、grocery，之後我們回家一起煮飯，今天煮了義大利麵，我和姊分工合作，我負責備料；姊負烹飪任，煮出來的紅醬我們都非常滿意！ 午餐結束後我們計畫搭公車前往Sunnyvale的租屋處打理環境以及設定網路等，查詢完通勤方式我們打算搭21號公車過去，將我們希望帶去的物品裝入小型行李箱後我們就出發了。 經過兩三站後，聰明的我們打開Google Map確認，果然非常的順利我們搭錯了方向，所以目的地改為前往Stanford！這就是為何我跟姊今天會拖著行李前往Stanford的原因。首先我們來到Palo Alto Art Center，(因為疫情，所以都只有在外面拍照採點，真的是非常的可惜，不然文青如我一定對許多藝術品能有一番評論)外圍環境就有著許多藝術品，十分讓人驚艷。 Main Quadrangle是西班牙風格的建築，走在其間真的讓我覺得這間學校太扯，做工非常精細，相當雅緻，走到最後面則可以看見Memorial Church，羅馬風格的天然雕刻石材、巨大柱子與低圓拱形和紅瓦屋頂，我只能以雄偉來形容，博物館跟教堂因為疫情無法參觀真的太可惜，因為這份可惜，我直接哭倒在教堂前面。 Hoover Tower為Stanford著名景點，無法上去一探灣區景色也十分可惜。在逛完學校的禮品店之後，我們搭乘Stanford的專車X離開學校回到Palo Alto轉運車站。前往Sunnyvale完成我們本該完成的任務。 今天可以說是千奇百怪異想天開的一天，但是卻非常充實，我跟姊都非常的開心，很期待在美國的每一天！(接下來停更兩天，直到去LA再來更新！)","link":"/2022/01/01/scenery-fo-america-week2/"}],"tags":[{"name":"RS","slug":"RS","link":"/tags/RS/"},{"name":"Recommender Systems","slug":"Recommender-Systems","link":"/tags/Recommender-Systems/"},{"name":"Diary","slug":"Diary","link":"/tags/Diary/"},{"name":"TCSE","slug":"TCSE","link":"/tags/TCSE/"}],"categories":[{"name":"筆記","slug":"筆記","link":"/categories/%E7%AD%86%E8%A8%98/"},{"name":"RS","slug":"筆記/RS","link":"/categories/%E7%AD%86%E8%A8%98/RS/"},{"name":"日記","slug":"日記","link":"/categories/%E6%97%A5%E8%A8%98/"},{"name":"Scenery of America","slug":"日記/Scenery-of-America","link":"/categories/%E6%97%A5%E8%A8%98/Scenery-of-America/"},{"name":"studies","slug":"studies","link":"/categories/studies/"}]}